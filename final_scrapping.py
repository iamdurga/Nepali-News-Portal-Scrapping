# -*- coding: utf-8 -*-
"""final Scrapping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17k_fP0gPqA5MLSjiIeUCN0ciBDXdTEnu

# Ekantipur
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from bs4 import BeautifulSoup as BS
import requests
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

http = urllib3.PoolManager()
http.addheaders = [('User-agent', 'Mozilla/61.0')]

news_values=[]
ndict = {'Title': [], "URL": [], "Date":[],
    "Author":[], "Author URL":[], "Content":[],"Category": [], "Description":[]}


categories={"news":"https://ekantipur.com/news/",
            "business":"https://ekantipur.com/business/",
            "world":"https://ekantipur.com/world/",
            "sports":"https://ekantipur.com/sports"
          }
show=False
for category, url in categories.items():
  web_page = http.request('GET', url)
  soup = BS(web_page.data, 'html5lib')

  for title in soup.findAll("h2"):
    if title.a:
      title_link=title.a.get("href")
      # print(title_link)
      if title_link.split(":")[0]!="https":
        title_link=url.split(f"/{category}")[0]+title.a.get("href")
      title_text=title.text
      #print(title_link)
      
      news_page = http.request('GET', title_link)
      news_soup = BS(news_page.data, 'html5lib')

      date = news_soup.find("time").text
      author_url = news_soup.select_one(".author").a.get("href")
      author_name = news_soup.select_one(".author").text

      for row in news_soup.select(".row"):
        for content in row.contents:
          if content.select(".normal"):
            content=content.p.text
            break
        break
      
      catagory = url.split('/')[-1]
# ndict = {'Title': [], "URL": [], "Date":[],
    # "Author":[], "Author URL":[], "Content":[],"Category": [], "Description":[]}

      ndict["Title"].append(title_text)
      ndict["URL"].append(title_link)
      ndict["Date"].append(date)
      ndict["Author"].append(author_name)
      ndict["Author URL"].append(author_url)
      ndict["Content"].append(content)
      ndict["Category"].append(category) 
      ndict["Description"].append(None)
      if show:
        print(f"""
                Title: {title_text}, URL: {title_link}
                Date: {date}, Author: {author_name},Category : {category},
                Author URL: {author_url},
                Content: {content}
                        """)
    # news_values.append()
ekantipur_df = pd.DataFrame(ndict, columns=list(ndict.keys()))
ekantipur_df

ekantipur_df.groupby(['URL']).count()

df=ekantipur_df.drop_duplicates(subset=["URL"])

df.groupby(['URL']).count()

"""# Gorkhapatra"""

categories={"national":"https://gorkhapatraonline.com/national",
            "international":"https://gorkhapatraonline.com/international",
           "society":"https://gorkhapatraonline.com/society", 
            "province": "https://gorkhapatraonline.com/province"
          }
ndict = {'Title': [], "URL": [], "Date":[],
      "Author":[], "Author URL":[], "Content":[],"Category": [], "Description":[]}

show=False
http = urllib3.PoolManager()
http.addheaders = [('User-agent', 'Mozilla/61.0')]

for category, url in categories.items():
  web_page = http.request('GET', url)
  soup = BS(web_page.data, 'html5lib')
  if category in ["national", "international"]:

    for news in soup.select(".business"):
      newsurl=news.find('a').get('href')
      trend2 = news.select_one(".trending2")
      try:
        title = trend2.find("p").text 
        title = title.strip()

        author = trend2.find('small').text
        author = author.strip()
        author = author.split('\xa0\xa0\xa0\xa0\n')[0]

        # author
        date = trend2.find('small').text
        date = date.strip()
        date = date.split('\xa0\xa0\xa0\xa0\n')[1]
        date=date.strip()
        description = trend2.select_one(".description").text.strip()
        
        # now got to this news url
        http.addheaders = [('User-agent', 'Mozilla/61.0')]
        web_page = http.request('GET',newsurl)
        news_soup = BS(web_page.data, 'html5lib')
        author_url = news_soup.select_one(".post-author-name").find("a").get("href")
        content=""
        for p in news_soup.select_one(".newstext").findAll("p"):
          content+="\n"+p.text
        # ndict["Title"].append(title)
        catagory = url.split("/")[-1]
        ndict["Title"].append(title)
        ndict["URL"].append(newsurl)
        ndict["Author"].append(author)
        ndict["Author URL"].append(author_url)
        ndict["Content"].append(content)
        ndict["Category"].append(category)
        ndict["Date"].append(date)
        ndict["Description"].append(description)
        if show:
          print(f"""
            Title: {title}, Title URL: {newsurl},
              Date: {date}, Author: {author},
              Author URL: {author_url}, 
              Category :{category} ,
            
              Content: {content},
              Description : {description}
                """)
        
      except:
        pass
  else:
    for feature1 in soup.select(".feature1"):
      # feature1 = soup.select_one(".feature1")
      newsurl = feature1.a.get("href")
      # news_url
      # title = 
      date=feature1.select_one(".feature-text").small.text.strip()
      title = feature1.select_one(".feature-text").text.strip().split("\n")[0]
      description = None

      category = url.split('/')[-1]

      news_page = http.request('GET', newsurl)
      news_soup = BS(news_page.data, "html5lib")
      # print(news_url)
      author_url = news_soup.select_one(".post-author-name").a.get("href")
      try:
        if len(news_soup.select_one(".newstext").find("p").find("strong").text.split(" ")) < 6:
          author =  news_soup.select_one(".newstext").find("p").find("strong").text
        else:
          author = None
        # print(author)
        # break
        content=""
        for p in news_soup.select_one(".newstext").findAll("p"):
          # print(p.text.split("गते । ")[-1])
          content+=p.text.split("गते । ")[-1]+"\n"

        ndict["Title"].append(title)
        ndict["URL"].append(newsurl)
        ndict["Author"].append(author)
        ndict["Author URL"].append(author_url)
        ndict["Content"].append(content)
        ndict["Category"].append(category)
        ndict["Date"].append(date)
        ndict["Description"].append(description)
        if show:
          print(f"""
            Title: {title}, Title URL: {newsurl},
              Date: {date}, Author: {author},
              Author URL: {author_url}, 
              Category :{category} ,
            
              Content: {content},
              Description : {description}
                """)
      
        
      except:
        pass
    # ndict = {'Title': [], "URL": [], "Date":[],
    # "Author":[], "Author URL":[], "Content":[],"Category": [], "Description":[]}

gorkhapatra_df = pd.DataFrame(ndict,columns = ndict.keys())    
gorkhapatra_df
# content must be done strip... \n is still there

gorkhapatra_df.groupby(['URL']).count()

"""# Annapurna Post"""

categories={"news":"https://www.onlinekhabar.com/content/news",
      "technology":"https://www.onlinekhabar.com/content/business/technology",
      "prabhas-news":"https://www.onlinekhabar.com/content/prabhas-news",
      "entertainment":"https://www.onlinekhabar.com/entertainment",
      "business":"https://www.onlinekhabar.com/business"
}
ndict = {'Title': [], "URL": [], "Date":[],
"Author":[], "Author URL":[], "Content":[],"Category": [], "Description":[]}

show=False
http = urllib3.PoolManager()
http.addheaders = [('User-agent', 'Mozilla/61.0')]

for category, url in categories.items():
  web_page = http.request('GET', url)
  soup = BS(web_page.data, 'html5lib')  
  if category in ["news", "technology","prabhas-news"]:
    for news in soup.select_one(".list_child_wrp").select(".title__regular"):
      
      newsurl = str(news).split('href="')[1].split('"')[0]
      try:
        title = news.text
        title = title.strip()
        author_url = None
        description = None

        news_page = http.request('GET', newsurl)
        news_soup = BS(news_page.data, "html5lib")
      
        if news_soup.select_one(".author__wrap"):
          author = news_soup.find("label").text
        else:
          author = None

      # if date in  news_soup.select(".post__time"):

        date=str(news_soup.select(".post__time")).split('<span>')[1].split('</span>')[0].split('मा')[0]

        content = " "
        for p in news_soup.select_one(".three__cols--grid").findAll("p"):
          content+=p.text.split("गते । ")[-1]+"\n"

        ndict["Title"].append(title)
        ndict["URL"].append(newsurl)
        ndict["Author"].append(author)
        ndict["Author URL"].append(author_url)
        ndict["Content"].append(content)
        ndict["Category"].append(category)
        ndict["Date"].append(date)
        ndict["Description"].append(description)
        if show:
          print(f"""
                Title: {title},  URL: {newsurl},
                Date: {date}, Author: {author},
                Author URL: {author_url}, 
                Category :{category} ,
              
                Content: {content},
                Description : {description}
                  """)

      except:
        pass

  else:
    description = None
    author_url = None

    for softwrap in soup.select(".soft__wrap"):
      if softwrap.find("h2"):
        newsurl = str(softwrap).split('href="')[1].split('"')[0]
        try:
          title = softwrap.find("h2").text
          title = title.strip()
          news_page = http.request('GET', newsurl)
          news_soup = BS(news_page.data, "html5lib")
          if news_soup.select_one(".author__wrap"):
            author = news_soup.find("label").text
          else:
            author = None
          date=str(news_soup.select(".post__time")).split('<span>')[1].split('</span>')[0].split('मा')[0]
          content = " "
          for p in news_soup.select_one(".ok__news--wrap").findAll("p"):
            content+=p.text+"\n"
        
          ndict["Title"].append(title)
          ndict["URL"].append(newsurl)
          ndict["Author"].append(author)
          ndict["Author URL"].append(author_url)
          ndict["Content"].append(content)
          ndict["Category"].append(category)
          ndict["Date"].append(date)
          ndict["Description"].append(description)
          if show:
              
            print(f"""
                  Title: {title}, URL: {newsurl},
                    Date: {date}, Author: {author},
                    Author URL: {Author_url}, 
                    Category :{category} ,
                  
                    Content: {content},
                    Description : {description}
                      """)
        except:
              pass

annapurna_df = pd.DataFrame(ndict,columns = ndict.keys())    
annapurna_df

annapurna_df.groupby(["URL"]).count()

df=annapurna_df.drop_duplicates(subset=["URL"]) 
# df

df.groupby(["URL"]).count()

annapurna_df[annapurna_df.URL=="https://www.onlinekhabar.com/2021/03/939687"]

annapurna_df[annapurna_df.URL=="https://www.onlinekhabar.com/2021/03/939689"]

annapurna_df[annapurna_df.URL=="https://www.onlinekhabar.com/2021/03/939690"]

annapurna_df[annapurna_df.URL=="https://www.onlinekhabar.com/2021/03/939695"]

df_row = pd.concat([ekantipur_df, gorkhapatra_df,annapurna_df])
df_row

from datetime import datetime as dt
datr=str(dt.now().date())

df_row.to_csv(f"{datr}.csv")

